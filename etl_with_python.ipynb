{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c74faa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob \n",
    "import pandas as pd \n",
    "import xml.etree.ElementTree as ET \n",
    "from datetime import datetime "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8142b4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = \"log_file.txt\"\n",
    "target_file = \"transformed_data.csv\"\n",
    "\n",
    "def extract_from_csv(file_to_process):\n",
    "    dataframe = pd.read_csv(file_to_process)\n",
    "    return dataframe\n",
    "\n",
    "def extract_from_json(file_to_process):\n",
    "    dataframe = pd.read_json(file_to_process, lines=True)\n",
    "    return dataframe\n",
    "\n",
    "def extract_from_xml(file_to_process):\n",
    "    dataframe = pd.DataFrame(columns=[\"name\", \"height\", \"weight\"])\n",
    "    tree = ET.parse(file_to_process)\n",
    "    root = tree.getroot()\n",
    "    for person in root:\n",
    "        name = person.find(\"name\").text\n",
    "        height = float(person.find(\"height\").text)\n",
    "        weight = float(person.find(\"weight\").text)\n",
    "        dataframe = pd.concat([dataframe, pd.DataFrame([{\"name\":name, \"height\":height, \"weight\":weight}])], ignore_index=True)\n",
    "    return dataframe\n",
    "\n",
    "def extract():\n",
    "    extracted_data = pd.DataFrame(columns=['name','height','weight']) # create an empty data frame to hold extracted data\n",
    "\n",
    "    # process all csv files, except the target file\n",
    "    for csvfile in glob.glob(\"*.csv\"):\n",
    "        if csvfile != target_file:  # check if the file is not the target file\n",
    "            extracted_data = pd.concat([extracted_data, pd.DataFrame(extract_from_csv(csvfile))], ignore_index=True)\n",
    "\n",
    "    # process all json files\n",
    "    for jsonfile in glob.glob(\"*.json\"):\n",
    "        extracted_data = pd.concat([extracted_data, pd.DataFrame(extract_from_json(jsonfile))], ignore_index=True)\n",
    "\n",
    "    # process all xml files\n",
    "    for xmlfile in glob.glob(\"*.xml\"):\n",
    "        extracted_data = pd.concat([extracted_data, pd.DataFrame(extract_from_xml(xmlfile))], ignore_index=True)\n",
    "\n",
    "    return extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ec2494f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(data):\n",
    "    '''Convert inches to meters and round off to two decimals\n",
    "    1 inch is 0.0254 meters '''\n",
    "    data['height'] = round(data.height * 0.0254,2)\n",
    "\n",
    "    '''Convert pounds to kilograms and round off to two decimals\n",
    "    1 pound is 0.45359237 kilograms '''\n",
    "    data['weight'] = round(data.weight * 0.45359237,2)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5941e39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(target_file, transformed_data):\n",
    "    transformed_data.to_csv(target_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c06daf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_progress(message):\n",
    "    timestamp_format = '%Y-%h-%d-%H:%M:%S' # Year-Monthname-Day-Hour-Minute-Second\n",
    "    now = datetime.now() # get current timestamp\n",
    "    timestamp = now.strftime(timestamp_format)\n",
    "    with open(log_file,\"a\") as f:\n",
    "        f.write(timestamp + ',' + message + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9ee8d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed Data\n",
      "     name  height  weight     1      Rav      Ahuja   TORONTO   CA\n",
      "0     NaN     NaN     NaN   2.0     Raul      Chong   Markham   CA\n",
      "1     NaN     NaN     NaN   3.0     Hima  Vasudevan   Chicago   US\n",
      "2     NaN     NaN     NaN   4.0     John     Thomas  Illinois   US\n",
      "3     NaN     NaN     NaN   5.0    Alice      James  Illinois   US\n",
      "4     NaN     NaN     NaN   6.0    Steve      Wells  Illinois   US\n",
      "5     NaN     NaN     NaN   7.0  Santosh      Kumar  Illinois   US\n",
      "6     NaN     NaN     NaN   8.0    Ahmed    Hussain  Illinois   US\n",
      "7     NaN     NaN     NaN   9.0    Nancy      Allen  Illinois   US\n",
      "8     NaN     NaN     NaN  10.0     Mary     Thomas  Illinois   US\n",
      "9     NaN     NaN     NaN  11.0  Bharath      Gupta  Illinois   US\n",
      "10    NaN     NaN     NaN  12.0   Andrea      Jones  Illinois   US\n",
      "11    NaN     NaN     NaN  13.0      Ann      Jacob  Illinois   US\n",
      "12    NaN     NaN     NaN  14.0     Amit      Kumar  NewDelhi   IN\n",
      "13   alex    1.67   51.25   NaN      NaN        NaN       NaN  NaN\n",
      "14   ajay    1.82   61.91   NaN      NaN        NaN       NaN  NaN\n",
      "15  alice    1.76   69.41   NaN      NaN        NaN       NaN  NaN\n",
      "16   ravi    1.73   64.56   NaN      NaN        NaN       NaN  NaN\n",
      "17    joe    1.72   65.45   NaN      NaN        NaN       NaN  NaN\n",
      "18   alex    1.67   51.25   NaN      NaN        NaN       NaN  NaN\n",
      "19   ajay    1.82   61.91   NaN      NaN        NaN       NaN  NaN\n",
      "20  alice    1.76   69.41   NaN      NaN        NaN       NaN  NaN\n",
      "21   ravi    1.73   64.56   NaN      NaN        NaN       NaN  NaN\n",
      "22    joe    1.72   65.45   NaN      NaN        NaN       NaN  NaN\n",
      "23   alex    1.67   51.25   NaN      NaN        NaN       NaN  NaN\n",
      "24   ajay    1.82   61.91   NaN      NaN        NaN       NaN  NaN\n",
      "25  alice    1.76   69.41   NaN      NaN        NaN       NaN  NaN\n",
      "26   ravi    1.73   64.56   NaN      NaN        NaN       NaN  NaN\n",
      "27    joe    1.72   65.45   NaN      NaN        NaN       NaN  NaN\n",
      "28   jack    1.74   55.93   NaN      NaN        NaN       NaN  NaN\n",
      "29    tom    1.77   64.18   NaN      NaN        NaN       NaN  NaN\n",
      "30  tracy    1.78   61.90   NaN      NaN        NaN       NaN  NaN\n",
      "31   john    1.72   50.97   NaN      NaN        NaN       NaN  NaN\n",
      "32   jack    1.74   55.93   NaN      NaN        NaN       NaN  NaN\n",
      "33    tom    1.77   64.18   NaN      NaN        NaN       NaN  NaN\n",
      "34  tracy    1.78   61.90   NaN      NaN        NaN       NaN  NaN\n",
      "35   john    1.72   50.97   NaN      NaN        NaN       NaN  NaN\n",
      "36   jack    1.74   55.93   NaN      NaN        NaN       NaN  NaN\n",
      "37    tom    1.77   64.18   NaN      NaN        NaN       NaN  NaN\n",
      "38  tracy    1.78   61.90   NaN      NaN        NaN       NaN  NaN\n",
      "39   john    1.72   50.97   NaN      NaN        NaN       NaN  NaN\n",
      "40  simon    1.72   50.97   NaN      NaN        NaN       NaN  NaN\n",
      "41  jacob    1.70   54.73   NaN      NaN        NaN       NaN  NaN\n",
      "42  cindy    1.69   57.81   NaN      NaN        NaN       NaN  NaN\n",
      "43   ivan    1.72   51.77   NaN      NaN        NaN       NaN  NaN\n",
      "44  simon    1.72   50.97   NaN      NaN        NaN       NaN  NaN\n",
      "45  jacob    1.70   54.73   NaN      NaN        NaN       NaN  NaN\n",
      "46  cindy    1.69   57.81   NaN      NaN        NaN       NaN  NaN\n",
      "47   ivan    1.72   51.77   NaN      NaN        NaN       NaN  NaN\n",
      "48  simon    1.72   50.97   NaN      NaN        NaN       NaN  NaN\n",
      "49  jacob    1.70   54.73   NaN      NaN        NaN       NaN  NaN\n",
      "50  cindy    1.69   57.81   NaN      NaN        NaN       NaN  NaN\n",
      "51   ivan    1.72   51.77   NaN      NaN        NaN       NaN  NaN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ErasmusHadebe\\AppData\\Local\\Temp\\ipykernel_26428\\647434673.py:29: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  extracted_data = pd.concat([extracted_data, pd.DataFrame(extract_from_csv(csvfile))], ignore_index=True)\n",
      "C:\\Users\\ErasmusHadebe\\AppData\\Local\\Temp\\ipykernel_26428\\647434673.py:20: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  dataframe = pd.concat([dataframe, pd.DataFrame([{\"name\":name, \"height\":height, \"weight\":weight}])], ignore_index=True)\n",
      "C:\\Users\\ErasmusHadebe\\AppData\\Local\\Temp\\ipykernel_26428\\647434673.py:20: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  dataframe = pd.concat([dataframe, pd.DataFrame([{\"name\":name, \"height\":height, \"weight\":weight}])], ignore_index=True)\n",
      "C:\\Users\\ErasmusHadebe\\AppData\\Local\\Temp\\ipykernel_26428\\647434673.py:20: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  dataframe = pd.concat([dataframe, pd.DataFrame([{\"name\":name, \"height\":height, \"weight\":weight}])], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Log the initialization of the ETL process\n",
    "log_progress(\"ETL Job Started\")\n",
    "\n",
    "# Log the beginning of the Extraction process\n",
    "log_progress(\"Extract phase Started\")\n",
    "extracted_data = extract()\n",
    "\n",
    "# Log the completion of the Extraction process\n",
    "log_progress(\"Extract phase Ended\")\n",
    "\n",
    "# Log the beginning of the Transformation process\n",
    "log_progress(\"Transform phase Started\")\n",
    "transformed_data = transform(extracted_data)\n",
    "print(\"Transformed Data\")\n",
    "print(transformed_data)\n",
    "\n",
    "# Log the completion of the Transformation process\n",
    "log_progress(\"Transform phase Ended\")\n",
    "\n",
    "# Log the beginning of the Loading process\n",
    "log_progress(\"Load phase Started\")\n",
    "load_data(target_file,transformed_data)\n",
    "\n",
    "# Log the completion of the Loading process\n",
    "log_progress(\"Load phase Ended\")\n",
    "\n",
    "# Log the completion of the ETL process\n",
    "log_progress(\"ETL Job Ended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ffa39b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOG: 2025-05-14-15:09:06,ETL Job Started\n",
      "LOG: 2025-05-14-15:09:06,Extract phase Started\n",
      "LOG: 2025-05-14-15:09:06,Found CSV files: ['INSTRUCTOR.csv', 'source1.csv', 'source2.csv', 'source3.csv', 'transformed_data.csv']\n",
      "Processing CSV: INSTRUCTOR.csv, Columns: ['1', 'Rav', 'Ahuja', 'TORONTO', 'CA']\n",
      "LOG: 2025-05-14-15:09:06,Skipping INSTRUCTOR.csv: Missing required columns ['name', 'height', 'weight']\n",
      "Processing CSV: source1.csv, Columns: ['name', 'height', 'weight']\n",
      "Extracted from source1.csv:\n",
      "    name  height  weight\n",
      "0   alex   65.78  112.99\n",
      "1   ajay   71.52  136.49\n",
      "2  alice   69.40  153.03\n",
      "3   ravi   68.22  142.34\n",
      "4    joe   67.79  144.30\n",
      "Processing CSV: source2.csv, Columns: ['name', 'height', 'weight']\n",
      "Extracted from source2.csv:\n",
      "    name  height  weight\n",
      "0   alex   65.78  112.99\n",
      "1   ajay   71.52  136.49\n",
      "2  alice   69.40  153.03\n",
      "3   ravi   68.22  142.34\n",
      "4    joe   67.79  144.30\n",
      "Processing CSV: source3.csv, Columns: ['name', 'height', 'weight']\n",
      "Extracted from source3.csv:\n",
      "    name  height  weight\n",
      "0   alex   65.78  112.99\n",
      "1   ajay   71.52  136.49\n",
      "2  alice   69.40  153.03\n",
      "3   ravi   68.22  142.34\n",
      "4    joe   67.79  144.30\n",
      "LOG: 2025-05-14-15:09:06,Found JSON files: ['source1.json', 'source2.json', 'source3.json']\n",
      "Processing JSON: source1.json, Columns: ['name', 'height', 'weight']\n",
      "Extracted from source1.json:\n",
      "    name  height  weight\n",
      "0   jack   68.70  123.30\n",
      "1    tom   69.80  141.49\n",
      "2  tracy   70.01  136.46\n",
      "3   john   67.90  112.37\n",
      "Processing JSON: source2.json, Columns: ['name', 'height', 'weight']\n",
      "Extracted from source2.json:\n",
      "    name  height  weight\n",
      "0   jack   68.70  123.30\n",
      "1    tom   69.80  141.49\n",
      "2  tracy   70.01  136.46\n",
      "3   john   67.90  112.37\n",
      "Processing JSON: source3.json, Columns: ['name', 'height', 'weight']\n",
      "Extracted from source3.json:\n",
      "    name  height  weight\n",
      "0   jack   68.70  123.30\n",
      "1    tom   69.80  141.49\n",
      "2  tracy   70.01  136.46\n",
      "3   john   67.90  112.37\n",
      "LOG: 2025-05-14-15:09:06,Found XML files: ['source1.xml', 'source2.xml', 'source3.xml']\n",
      "Processing XML: source1.xml, Data:\n",
      "    name  height  weight\n",
      "0  simon   67.90  112.37\n",
      "1  jacob   66.78  120.67\n",
      "2  cindy   66.49  127.45\n",
      "3   ivan   67.62  114.14\n",
      "Processing XML: source2.xml, Data:\n",
      "    name  height  weight\n",
      "0  simon   67.90  112.37\n",
      "1  jacob   66.78  120.67\n",
      "2  cindy   66.49  127.45\n",
      "3   ivan   67.62  114.14\n",
      "Processing XML: source3.xml, Data:\n",
      "    name  height  weight\n",
      "0  simon   67.90  112.37\n",
      "1  jacob   66.78  120.67\n",
      "2  cindy   66.49  127.45\n",
      "3   ivan   67.62  114.14\n",
      "LOG: 2025-05-14-15:09:06,Extracted Data (before transform):\n",
      "     name  height  weight\n",
      "0    alex   65.78  112.99\n",
      "1    ajay   71.52  136.49\n",
      "2   alice   69.40  153.03\n",
      "3    ravi   68.22  142.34\n",
      "4     joe   67.79  144.30\n",
      "5    alex   65.78  112.99\n",
      "6    ajay   71.52  136.49\n",
      "7   alice   69.40  153.03\n",
      "8    ravi   68.22  142.34\n",
      "9     joe   67.79  144.30\n",
      "10   alex   65.78  112.99\n",
      "11   ajay   71.52  136.49\n",
      "12  alice   69.40  153.03\n",
      "13   ravi   68.22  142.34\n",
      "14    joe   67.79  144.30\n",
      "15   jack   68.70  123.30\n",
      "16    tom   69.80  141.49\n",
      "17  tracy   70.01  136.46\n",
      "18   john   67.90  112.37\n",
      "19   jack   68.70  123.30\n",
      "20    tom   69.80  141.49\n",
      "21  tracy   70.01  136.46\n",
      "22   john   67.90  112.37\n",
      "23   jack   68.70  123.30\n",
      "24    tom   69.80  141.49\n",
      "25  tracy   70.01  136.46\n",
      "26   john   67.90  112.37\n",
      "27  simon   67.90  112.37\n",
      "28  jacob   66.78  120.67\n",
      "29  cindy   66.49  127.45\n",
      "30   ivan   67.62  114.14\n",
      "31  simon   67.90  112.37\n",
      "32  jacob   66.78  120.67\n",
      "33  cindy   66.49  127.45\n",
      "34   ivan   67.62  114.14\n",
      "35  simon   67.90  112.37\n",
      "36  jacob   66.78  120.67\n",
      "37  cindy   66.49  127.45\n",
      "38   ivan   67.62  114.14\n",
      "LOG: 2025-05-14-15:09:06,Extract phase Ended\n",
      "LOG: 2025-05-14-15:09:06,Transform phase Started\n",
      "LOG: 2025-05-14-15:09:06,Transformed Data:\n",
      "     name  height  weight\n",
      "0    alex    1.67   51.25\n",
      "1    ajay    1.82   61.91\n",
      "2   alice    1.76   69.41\n",
      "3    ravi    1.73   64.56\n",
      "4     joe    1.72   65.45\n",
      "5    alex    1.67   51.25\n",
      "6    ajay    1.82   61.91\n",
      "7   alice    1.76   69.41\n",
      "8    ravi    1.73   64.56\n",
      "9     joe    1.72   65.45\n",
      "10   alex    1.67   51.25\n",
      "11   ajay    1.82   61.91\n",
      "12  alice    1.76   69.41\n",
      "13   ravi    1.73   64.56\n",
      "14    joe    1.72   65.45\n",
      "15   jack    1.74   55.93\n",
      "16    tom    1.77   64.18\n",
      "17  tracy    1.78   61.90\n",
      "18   john    1.72   50.97\n",
      "19   jack    1.74   55.93\n",
      "20    tom    1.77   64.18\n",
      "21  tracy    1.78   61.90\n",
      "22   john    1.72   50.97\n",
      "23   jack    1.74   55.93\n",
      "24    tom    1.77   64.18\n",
      "25  tracy    1.78   61.90\n",
      "26   john    1.72   50.97\n",
      "27  simon    1.72   50.97\n",
      "28  jacob    1.70   54.73\n",
      "29  cindy    1.69   57.81\n",
      "30   ivan    1.72   51.77\n",
      "31  simon    1.72   50.97\n",
      "32  jacob    1.70   54.73\n",
      "33  cindy    1.69   57.81\n",
      "34   ivan    1.72   51.77\n",
      "35  simon    1.72   50.97\n",
      "36  jacob    1.70   54.73\n",
      "37  cindy    1.69   57.81\n",
      "38   ivan    1.72   51.77\n",
      "Final Transformed Data:\n",
      "     name  height  weight\n",
      "0    alex    1.67   51.25\n",
      "1    ajay    1.82   61.91\n",
      "2   alice    1.76   69.41\n",
      "3    ravi    1.73   64.56\n",
      "4     joe    1.72   65.45\n",
      "5    alex    1.67   51.25\n",
      "6    ajay    1.82   61.91\n",
      "7   alice    1.76   69.41\n",
      "8    ravi    1.73   64.56\n",
      "9     joe    1.72   65.45\n",
      "10   alex    1.67   51.25\n",
      "11   ajay    1.82   61.91\n",
      "12  alice    1.76   69.41\n",
      "13   ravi    1.73   64.56\n",
      "14    joe    1.72   65.45\n",
      "15   jack    1.74   55.93\n",
      "16    tom    1.77   64.18\n",
      "17  tracy    1.78   61.90\n",
      "18   john    1.72   50.97\n",
      "19   jack    1.74   55.93\n",
      "20    tom    1.77   64.18\n",
      "21  tracy    1.78   61.90\n",
      "22   john    1.72   50.97\n",
      "23   jack    1.74   55.93\n",
      "24    tom    1.77   64.18\n",
      "25  tracy    1.78   61.90\n",
      "26   john    1.72   50.97\n",
      "27  simon    1.72   50.97\n",
      "28  jacob    1.70   54.73\n",
      "29  cindy    1.69   57.81\n",
      "30   ivan    1.72   51.77\n",
      "31  simon    1.72   50.97\n",
      "32  jacob    1.70   54.73\n",
      "33  cindy    1.69   57.81\n",
      "34   ivan    1.72   51.77\n",
      "35  simon    1.72   50.97\n",
      "36  jacob    1.70   54.73\n",
      "37  cindy    1.69   57.81\n",
      "38   ivan    1.72   51.77\n",
      "LOG: 2025-05-14-15:09:06,Transform phase Ended\n",
      "LOG: 2025-05-14-15:09:06,Load phase Started\n",
      "LOG: 2025-05-14-15:09:06,Saved to transformed_data.csv\n",
      "LOG: 2025-05-14-15:09:06,Saved to STAFF.db, table 'People'\n",
      "LOG: 2025-05-14-15:09:06,Load phase Ended\n",
      "LOG: 2025-05-14-15:09:06,ETL Job Ended\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ErasmusHadebe\\AppData\\Local\\Temp\\ipykernel_26428\\1454186235.py:99: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  extracted_data = pd.concat([extracted_data, extract_from_csv(csvfile)], ignore_index=True)\n",
      "C:\\Users\\ErasmusHadebe\\AppData\\Local\\Temp\\ipykernel_26428\\1454186235.py:87: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  dataframe = pd.concat([dataframe, pd.DataFrame([{\"name\": name, \"height\": height, \"weight\": weight}])], ignore_index=True)\n",
      "C:\\Users\\ErasmusHadebe\\AppData\\Local\\Temp\\ipykernel_26428\\1454186235.py:87: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  dataframe = pd.concat([dataframe, pd.DataFrame([{\"name\": name, \"height\": height, \"weight\": weight}])], ignore_index=True)\n",
      "C:\\Users\\ErasmusHadebe\\AppData\\Local\\Temp\\ipykernel_26428\\1454186235.py:87: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  dataframe = pd.concat([dataframe, pd.DataFrame([{\"name\": name, \"height\": height, \"weight\": weight}])], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from datetime import datetime\n",
    "import sqlite3\n",
    "\n",
    "log_file = \"log_file.txt\"\n",
    "target_file = \"transformed_data.csv\"\n",
    "db_name = \"STAFF.db\"  # Connect to STAFF.db from your earlier question\n",
    "\n",
    "def log_progress(message):\n",
    "    timestamp_format = '%Y-%m-%d-%H:%M:%S'  # Changed %h to %m for numeric month\n",
    "    now = datetime.now()\n",
    "    timestamp = now.strftime(timestamp_format)\n",
    "    log_message = f\"{timestamp},{message}\"\n",
    "    print(f\"LOG: {log_message}\")  # Print to terminal for visibility\n",
    "    with open(log_file, \"a\") as f:\n",
    "        f.write(log_message + '\\n')\n",
    "\n",
    "def extract_from_csv(file_to_process):\n",
    "    try:\n",
    "        dataframe = pd.read_csv(file_to_process)\n",
    "        print(f\"Processing CSV: {file_to_process}, Columns: {list(dataframe.columns)}\")\n",
    "\n",
    "        # Map columns (adjust based on your data)\n",
    "        column_mapping = {\n",
    "            'FirstName': 'name',  # Combine FirstName and LastName if needed\n",
    "            'Height': 'height',\n",
    "            'Weight': 'weight'\n",
    "        }\n",
    "        # If FirstName and LastName exist, combine them into name\n",
    "        if 'FirstName' in dataframe.columns and 'LastName' in dataframe.columns:\n",
    "            dataframe['name'] = dataframe['FirstName'] + ' ' + dataframe['LastName']\n",
    "        elif 'FirstName' in dataframe.columns:\n",
    "            dataframe['name'] = dataframe['FirstName']\n",
    "\n",
    "        # Rename columns and select only name, height, weight\n",
    "        dataframe = dataframe.rename(columns=column_mapping)\n",
    "        required_columns = ['name', 'height', 'weight']\n",
    "        if not all(col in dataframe.columns for col in required_columns):\n",
    "            log_progress(f\"Skipping {file_to_process}: Missing required columns {required_columns}\")\n",
    "            return pd.DataFrame(columns=required_columns)\n",
    "        dataframe = dataframe[required_columns]\n",
    "        print(f\"Extracted from {file_to_process}:\\n{dataframe}\")\n",
    "        return dataframe\n",
    "    except Exception as e:\n",
    "        log_progress(f\"Error processing CSV {file_to_process}: {str(e)}\")\n",
    "        return pd.DataFrame(columns=['name', 'height', 'weight'])\n",
    "\n",
    "def extract_from_json(file_to_process):\n",
    "    try:\n",
    "        dataframe = pd.read_json(file_to_process, lines=True)\n",
    "        print(f\"Processing JSON: {file_to_process}, Columns: {list(dataframe.columns)}\")\n",
    "\n",
    "        # Map columns\n",
    "        column_mapping = {\n",
    "            'FirstName': 'name',\n",
    "            'Height': 'height',\n",
    "            'Weight': 'weight'\n",
    "        }\n",
    "        if 'FirstName' in dataframe.columns and 'LastName' in dataframe.columns:\n",
    "            dataframe['name'] = dataframe['FirstName'] + ' ' + dataframe['LastName']\n",
    "        elif 'FirstName' in dataframe.columns:\n",
    "            dataframe['name'] = dataframe['FirstName']\n",
    "\n",
    "        dataframe = dataframe.rename(columns=column_mapping)\n",
    "        required_columns = ['name', 'height', 'weight']\n",
    "        if not all(col in dataframe.columns for col in required_columns):\n",
    "            log_progress(f\"Skipping {file_to_process}: Missing required columns {required_columns}\")\n",
    "            return pd.DataFrame(columns=required_columns)\n",
    "        dataframe = dataframe[required_columns]\n",
    "        print(f\"Extracted from {file_to_process}:\\n{dataframe}\")\n",
    "        return dataframe\n",
    "    except Exception as e:\n",
    "        log_progress(f\"Error processing JSON {file_to_process}: {str(e)}\")\n",
    "        return pd.DataFrame(columns=['name', 'height', 'weight'])\n",
    "\n",
    "def extract_from_xml(file_to_process):\n",
    "    try:\n",
    "        dataframe = pd.DataFrame(columns=[\"name\", \"height\", \"weight\"])\n",
    "        tree = ET.parse(file_to_process)\n",
    "        root = tree.getroot()\n",
    "        for person in root:\n",
    "            name = person.find(\"name\").text\n",
    "            height = float(person.find(\"height\").text)\n",
    "            weight = float(person.find(\"weight\").text)\n",
    "            dataframe = pd.concat([dataframe, pd.DataFrame([{\"name\": name, \"height\": height, \"weight\": weight}])], ignore_index=True)\n",
    "        print(f\"Processing XML: {file_to_process}, Data:\\n{dataframe}\")\n",
    "        return dataframe\n",
    "    except Exception as e:\n",
    "        log_progress(f\"Error processing XML {file_to_process}: {str(e)}\")\n",
    "        return pd.DataFrame(columns=['name', 'height', 'weight'])\n",
    "\n",
    "def extract():\n",
    "    extracted_data = pd.DataFrame(columns=['name', 'height', 'weight'])\n",
    "    log_progress(f\"Found CSV files: {glob.glob('*.csv')}\")\n",
    "    for csvfile in glob.glob(\"*.csv\"):\n",
    "        if csvfile != target_file:\n",
    "            extracted_data = pd.concat([extracted_data, extract_from_csv(csvfile)], ignore_index=True)\n",
    "\n",
    "    log_progress(f\"Found JSON files: {glob.glob('*.json')}\")\n",
    "    for jsonfile in glob.glob(\"*.json\"):\n",
    "        extracted_data = pd.concat([extracted_data, extract_from_json(jsonfile)], ignore_index=True)\n",
    "\n",
    "    log_progress(f\"Found XML files: {glob.glob('*.xml')}\")\n",
    "    for xmlfile in glob.glob(\"*.xml\"):\n",
    "        extracted_data = pd.concat([extracted_data, extract_from_xml(xmlfile)], ignore_index=True)\n",
    "\n",
    "    log_progress(f\"Extracted Data (before transform):\\n{extracted_data}\")\n",
    "    return extracted_data\n",
    "\n",
    "def transform(data):\n",
    "    if data.empty:\n",
    "        log_progress(\"No data to transform\")\n",
    "        return data\n",
    "    data = data.dropna(subset=['name', 'height', 'weight'])  # Remove rows with NaN\n",
    "    data['height'] = round(data.height * 0.0254, 2)  # Inches to meters\n",
    "    data['weight'] = round(data.weight * 0.45359237, 2)  # Pounds to kilograms\n",
    "    log_progress(f\"Transformed Data:\\n{data}\")\n",
    "    return data\n",
    "\n",
    "def load_data(target_file, transformed_data, db_name):\n",
    "    if transformed_data.empty:\n",
    "        log_progress(\"No data to load\")\n",
    "        return\n",
    "    transformed_data.to_csv(target_file, index=False)\n",
    "    log_progress(f\"Saved to {target_file}\")\n",
    "    # Save to SQLite database\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_name)\n",
    "        transformed_data.to_sql('People', conn, if_exists='replace', index=False)\n",
    "        log_progress(f\"Saved to {db_name}, table 'People'\")\n",
    "        conn.close()\n",
    "    except Exception as e:\n",
    "        log_progress(f\"Error saving to database {db_name}: {str(e)}\")\n",
    "\n",
    "# ETL Process\n",
    "log_progress(\"ETL Job Started\")\n",
    "log_progress(\"Extract phase Started\")\n",
    "extracted_data = extract()\n",
    "log_progress(\"Extract phase Ended\")\n",
    "log_progress(\"Transform phase Started\")\n",
    "transformed_data = transform(extracted_data)\n",
    "print(\"Final Transformed Data:\")\n",
    "print(transformed_data)\n",
    "log_progress(\"Transform phase Ended\")\n",
    "log_progress(\"Load phase Started\")\n",
    "load_data(target_file, transformed_data, db_name)\n",
    "log_progress(\"Load phase Ended\")\n",
    "log_progress(\"ETL Job Ended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4481cd27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
